{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8lbClye5K7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Data**"
      ],
      "metadata": {
        "id": "vZlNrmTm5MSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PY-rwERb2XD2"
      },
      "outputs": [],
      "source": [
        "#Training data\n",
        "\n",
        "import csv\n",
        "\n",
        "# Initialize an empty dictionary\n",
        "train_data_dict = {}\n",
        "train_data_dict[\"Sentence\"]=[]\n",
        "train_data_dict[\"Toxic_spans\"]=[]\n",
        "train_data_dict[\"Label\"]=[]\n",
        "\n",
        "# Open the CSV file\n",
        "with open('tsd_train.csv', mode='r') as file:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(file)\n",
        "    reader=list(reader)\n",
        "    # Iterate over each row in the CSV file\n",
        "    for i in range(1,len(reader)):\n",
        "        row=reader[i]\n",
        "        #print(row)\n",
        "        train_data_dict[\"Sentence\"].append(row[1])\n",
        "        s=row[0][1:]\n",
        "        s=s[:-1]\n",
        "        span_list=s.split(',')\n",
        "        span_list=[''.join(j.split()) for j in span_list]\n",
        "        #print(span_list)\n",
        "        span_list=[j for j in span_list if j!='']\n",
        "        span_list=[int(j) for j in span_list]\n",
        "        train_data_dict[\"Toxic_spans\"].append(span_list)\n",
        "        if row[0]!=\"[]\":\n",
        "            train_data_dict[\"Label\"].append('toxic')\n",
        "        else:\n",
        "            train_data_dict[\"Label\"].append(\"not toxic\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_dict[\"Label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7AoSbcb3CUd",
        "outputId": "16edc714-aa94-4ada-f11d-e2e571d535a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7939"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dict[\"Toxic_spans\"]"
      ],
      "metadata": {
        "id": "zQvgjSYX6iFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dict[\"Label\"].count('toxic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQF8sZEq6_KE",
        "outputId": "faae7e78-286c-41e7-b1a8-6b313d78fae3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7454"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9x_m7PuB6iI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyMNh25r9AA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5HiquW-90-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BIO Encoding"
      ],
      "metadata": {
        "id": "v5CQB1o691uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BIO_encoding(input_data):\n",
        "    Encoded_dict={}     #Dictionary to store BIO encoding\n",
        "\n",
        "    for i in range(len(input_data[\"Sentence\"])):\n",
        "        d={}\n",
        "        d[\"Text\"]=input_data[\"Sentence\"][i]\n",
        "        Bio_labels=[]   #store BIO code\n",
        "        if input_data[\"Label\"][i]=='toxic':\n",
        "            indices=input_data[\"Toxic_spans\"][i]\n",
        "\n",
        "            #Currently assuming the toxic span is continuous(CHECK THIS AND CHANGE IF NEEDED)\n",
        "            #print(d[\"Text\"])\n",
        "            #print(indices)\n",
        "            prev=d[\"Text\"][:indices[0]].split()     #part before toxic span\n",
        "            Bio_labels+=['O']*len(prev)     #O label for non-toxic part\n",
        "\n",
        "            #now for toxic part\n",
        "            k=len(indices)\n",
        "            start=0\n",
        "            end=1\n",
        "            prev_ind=indices[start]\n",
        "            while end<k:\n",
        "                if indices[end]>prev_ind+1:\n",
        "                    #print(start,end,prev_ind)\n",
        "                    span=d[\"Text\"][indices[start]:(indices[end-1]+1)]    #extract that span of text\n",
        "                    tokens=span.split()     #break them into tokens/words\n",
        "                    next=d[\"Text\"][indices[end-1]+1:indices[end]].split()  #part after span\n",
        "                    Bio_labels.append('B')          #B label to start span\n",
        "                    Bio_labels+=['I']*(len(tokens)-1)   #I labels for rest of the span\n",
        "                    Bio_labels+=['O']*len(next)     #O labels for remaining non-toxic part\n",
        "                    start=end\n",
        "                    end=start+1\n",
        "                    prev_ind=indices[start]\n",
        "                else:\n",
        "                    end+=1\n",
        "                    prev_ind+=1\n",
        "            #print(indices)\n",
        "            #print(start,end)\n",
        "            span=d[\"Text\"][indices[start]:(indices[end-1]+1)]    #extract that span of text\n",
        "            tokens=span.split()     #break them into tokens/words\n",
        "\n",
        "            Bio_labels.append('B')          #B label to start span\n",
        "            Bio_labels+=['I']*(len(tokens)-1)   #I labels for rest of the span\n",
        "            next=d[\"Text\"][indices[end-1]+1:].split()  #part after span\n",
        "            Bio_labels+=['O']*len(next)     #O labels for remaining non-toxic part\n",
        "\n",
        "        else:\n",
        "            Bio_labels=['O']*len(d[\"Text\"].split())\n",
        "\n",
        "        d[\"Bio_labels\"]=Bio_labels  #add to dict\n",
        "        Encoded_dict[i+1]=d\n",
        "\n",
        "    return Encoded_dict\n"
      ],
      "metadata": {
        "id": "tmkSe4ZG91Bw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2dN02lp1A41o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BIO_codes_train=BIO_encoding(train_data_dict)"
      ],
      "metadata": {
        "id": "MQD5tn4LA445"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZOFDi6iB4kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BIO_codes_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHb7GsklB4nw",
        "outputId": "c0ab9458-c379-4441-b62d-f9cb19e3a929"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Text': 'Another violent and aggressive immigrant killing a innocent and intelligent US Citizen.... Sarcasm',\n",
              " 'Bio_labels': ['O',\n",
              "  'B',\n",
              "  'I',\n",
              "  'I',\n",
              "  'I',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BIO_codes_train[9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-eZOPS7CeUI",
        "outputId": "c4a806b3-1072-4cc8-acec-2c790483d3fa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Text': \"Obamacare is on it's last gasping breaths.   You idiots who don't want something else passed that's going to save your free healthcare are kind of stupid.   But if you block anything else that gives you socialized healthcare it's your own fault for being unbudging.  You can go back to nothing when nothing further is passed and Obamacare is just an ugly memory.\",\n",
              " 'Bio_labels': ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQXWJ3t_eiaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "talybUYdeidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save encoded dictionary\n",
        "\n",
        "import json\n",
        "train_set=json.dumps(BIO_codes_train,indent=4)\n",
        "\n",
        "with open('BIO_train.json','w') as f:\n",
        "    f.write(train_set)"
      ],
      "metadata": {
        "id": "LNlKSjLZeigf"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}